{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is belong to [here]( https://www.kaggle.com/competitions/amex-default-prediction/discussion/329436)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データ重すぎてメモリに乗らない。圧縮方法まとめは[ここ](https://www.kaggle.com/competitions/amex-default-prediction/discussion/328054)。+[補足](https://www.kaggle.com/competitions/amex-default-prediction/discussion/330347)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自作データ読み込み \n",
    "## 背景(課題)\n",
    "1. メモリ\n",
    "→ 軽くすると計算速度・メモリ消費量が改善される。\n",
    "2. 読み込み\n",
    "→ 実行が早くなる。\n",
    "\n",
    "## 実装\n",
    "pickleを読み込んでdtypeをfloat64->float32変更してdfを返す。\n",
    "\n",
    "## 課題\n",
    "* pickleがあることを前提として読み込み部分を実装している。→csv_to_pickleは必要?  \n",
    "* dtype変更はfloat64の特徴量をeda.pyにべた書きしている。→ 型を勝手に読み込んで特徴をピックアップする？これをやると64→32をやってもいいのかという判定が必要で面倒そう.\n",
    "* 分割されたpickleファイルをまとめて読み込むようにする。以下イメージ\n",
    "\n",
    "```\n",
    "\n",
    "df = eda.read_pickles(dirname)\n",
    "\n",
    "dirname = 'pickle/'\n",
    "\n",
    "pickle  --- train_1.pickle\n",
    "        | - train_2.pickle\n",
    "        | - train_3.pickle\n",
    "        | - train_4.pickle\n",
    "        .\n",
    "        .\n",
    "        .\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000000 entries, 0 to 999999\n",
      "Columns: 190 entries, customer_ID to D_145\n",
      "dtypes: float32(185), int64(1), object(4)\n",
      "memory usage: 743.9+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000000 entries, 0 to 999999\n",
      "Columns: 190 entries, customer_ID to D_145\n",
      "dtypes: float64(185), int64(1), object(4)\n",
      "memory usage: 1.4+ GB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import eda\n",
    "import pandas as pd\n",
    "#df = pd.read_csv('dataset/train_data.csv')\n",
    "pickle = 'pickle/train_data_0.pkl'\n",
    "df1 = eda.read_pickle(filename=pickle)\n",
    "print(df1.info())\n",
    "df2 = pd.read_pickle(pickle)\n",
    "print(df2.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Feature Engineering ([参考](https://www.kaggle.com/competitions/ieee-fraud-detection/discussion/108575))\n",
    "### Train and Test\n",
    "訓練とテストを分離する。今回は不要。\n",
    "### NAN processing\n",
    "nanの処理。参考文献では例として-999を代入している。LGBM(=Light GBM?)だとアルゴリズムがうまく捌いてくれるらしい。\n",
    "### Label Encode/ Factorize/ Memory reduction\n",
    "全体を通して文字列特徴量の扱い方を説明している。\n",
    "* pandasのfactorizeを使うと文字列をa→1, b→2, c→3...のように変換してくれる。([例](https://qiita.com/QUANON/items/08a65012366abd150172))\n",
    "* データタイプを落とすとデータの圧縮になる(float64→float32, int64→int32,...)  \n",
    "\n",
    "### Categorical Features\n",
    "LGBMはカテゴリ変数を学習可能なので、df.astype('category')のように型を指定してもよい。(ラベルエンコードしない場合)\n",
    "\n",
    "### Splitting\n",
    "主に文字列特徴量で、\"Mac OS X 10_9_5\"のような特徴量を\"Mac OS X\"(OS名)と\"10_9_5\"(バージョン)に分離。\n",
    "\n",
    "### Combining / Transforming / Interaction\n",
    "* 文字列特徴量であれば連結できる. \n",
    "```\n",
    "df[new_column] = df[str_column] + df[str_column2]\n",
    "```\n",
    "* 数値特徴量であれば計算して利用できる.\n",
    "```\n",
    "df[new_column] = df[int_column] * df[int_column2]\n",
    "```\n",
    "\n",
    "### Frequency Encoding\n",
    "カテゴリ変数をデータ全体での頻出回数でエンコードする方法。\n",
    "\n",
    "### Aggregations / Group Statistics\n",
    "```\n",
    "temp = df.groupby('card1')['TransactionAmt'].agg(['mean'])   \n",
    " .rename({'mean':'TransactionAmt_card1_mean'},axis=1)\n",
    "df = pd.merge(df,temp,on='card1',how='left')\n",
    "```\n",
    "カテゴリ変数でグループ化して数値特徴量を加工(変換)する。  \n",
    "上の式では、'card1'でグループ化して'TransactionAmt'の平均値を新規特徴量として生成している。\n",
    "```\n",
    "   card1 TransactionAmt TransactionAmt_card1_mean\n",
    " 0 a     2              2.5\n",
    " 1 b     4              4\n",
    " 2 a     3              2.5\n",
    "```\n",
    "参考: [groupby](https://qiita.com/propella/items/a9a32b878c77222630ae), [agg](https://note.nkmk.me/python-pandas-agg-aggregate/)\n",
    "\n",
    "### Normalize / Standardize\n",
    "標準化・正規化(省略)\n",
    "\n",
    "### Outlier Removal / Relax / Smooth / PCA\n",
    "異常値の削除。この記事のコンテストでは異常値を発見したかったそうで、平滑化(Relax?)を使用。頻出度が全体の0.1%未満の特徴量(カテゴリ変数の話っぽい)を-9999に変換するなど."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More about Aggregations / Group Statistics\n",
    "上のAggregations重要って言ってるだけっぽい。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bruteforce Feature Engineering\n",
    "足して引いてかけて割って相関高いやつを消す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルたくさん作って予測値を線形回帰とかでくっつける。特徴量関係ないじゃん"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudolabeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル作ってテストデータを予測。予測した値を疑似ラベルとしてトレインデータにする。特徴量関係ないじゃん"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
